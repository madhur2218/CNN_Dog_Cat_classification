{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf98dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator #for preprocessing of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f17a1ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ea296",
   "metadata": {},
   "source": [
    "# Preorocessing of image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365d4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing training set (inorder to avoid overfitting of data)\n",
    "#Image augmentation (changing various aspects of the image)\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,            #notmalizing data vectors between 0 and 1\n",
    "                                   shear_range = 0.2,          #sheering the image by 20 percent     \n",
    "                                   zoom_range = 0.2,          # zooing in and out by 20 percent\n",
    "                                   horizontal_flip = True)   #horizontally and vertically rotating the image \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5643e436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('C:/Users/Madhur/Desktop/Python/CNN/Dog cat classification/dataset/training_set',  \n",
    "                                                 target_size = (64, 64), \n",
    "                                                 batch_size = 32,       \n",
    "                                                 class_mode = 'binary') \n",
    "\n",
    "\n",
    "#implementing the above task on out dataset\n",
    "#target_size == final size of images, we can also increases the size of images but it can lead to increase in traing time of images \n",
    "# batch_size== how many imaeges we want to have in one set of batch for training purpose, 32 is the default value\n",
    "#class_mode == binary classification as we have two classes dog and cat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4c4580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing testing set data\n",
    "# Preprocessing the Test set\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('C:/Users/Madhur/Desktop/Python/CNN/Dog cat classification/dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fac687",
   "metadata": {},
   "source": [
    "# Building Convolution Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a85301",
   "metadata": {},
   "source": [
    "Step 1 : Initializing CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68d8cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=tf.keras.models.Sequential() #Sequential means a sequence of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2ed4d",
   "metadata": {},
   "source": [
    "Step 2 : Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4696af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3,activation= 'relu', input_shape=[64,64,3]))\n",
    "#filters== is the total number of filters used in this layer\n",
    "#kernel_size== it is the size of the filters, here we have taken a 3x3 matrix filter size\n",
    "#activation function== reul act fn used \n",
    "#input_shape== size of the shape of colored images provided, if we were using grayscale images then we would have written 1 insted of 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f09915",
   "metadata": {},
   "source": [
    "Step 3 : Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d15fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ae8d8",
   "metadata": {},
   "source": [
    "Adding a Second Convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c52ac7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "        \n",
    "        #Here we do not write the input_shape as we already did in the first input layer and doing this could lead to a poor model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45b486d",
   "metadata": {},
   "source": [
    "Adding Third Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea6b7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd529229",
   "metadata": {},
   "source": [
    "Adding Fourth Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "025e23ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"conv2d_6\" (type Conv2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node conv2d_6/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_6/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,32], [3,3,32,32].\n\nCall arguments received by layer \"conv2d_6\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 32), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m cnn\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mMaxPool2D(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:587\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 587\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    589\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1963\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1960\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1961\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1962\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c_op\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"conv2d_6\" (type Conv2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node conv2d_6/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_6/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,32], [3,3,32,32].\n\nCall arguments received by layer \"conv2d_6\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 32), dtype=float32)"
     ]
    }
   ],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf1654",
   "metadata": {},
   "source": [
    "Adding Fifth Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5531a791",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"conv2d_7\" (type Conv2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node conv2d_7/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_7/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,32], [3,3,32,32].\n\nCall arguments received by layer \"conv2d_7\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 32), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m cnn\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mMaxPool2D(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:587\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 587\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    589\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1963\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1960\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1961\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1962\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c_op\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"conv2d_7\" (type Conv2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node conv2d_7/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_7/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,32], [3,3,32,32].\n\nCall arguments received by layer \"conv2d_7\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 32), dtype=float32)"
     ]
    }
   ],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc5da4",
   "metadata": {},
   "source": [
    "Step 4 : Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8592b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6362681",
   "metadata": {},
   "source": [
    "Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b03bda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining all the above steps \n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "#units==Units are the number of neurons contained in each layer\n",
    "#activation== relu used\n",
    "# a dense layer is a layer that is deeply connected with its preceding layer which means the neurons of the layer are connected to every neuron of its preceding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68088b4f",
   "metadata": {},
   "source": [
    "Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b07d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feeb47d",
   "metadata": {},
   "source": [
    "Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9bf3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the cnn\n",
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea331a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.6794 - accuracy: 0.5543 - val_loss: 0.6499 - val_accuracy: 0.6130\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.6482 - accuracy: 0.6226 - val_loss: 0.5895 - val_accuracy: 0.6955\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.6018 - accuracy: 0.6699 - val_loss: 0.5492 - val_accuracy: 0.7325\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.5716 - accuracy: 0.7041 - val_loss: 0.5759 - val_accuracy: 0.7015\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 0.5501 - accuracy: 0.7170 - val_loss: 0.5152 - val_accuracy: 0.7430\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 0.5182 - accuracy: 0.7473 - val_loss: 0.6179 - val_accuracy: 0.6950\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.4971 - accuracy: 0.7580 - val_loss: 0.4676 - val_accuracy: 0.7755\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.4809 - accuracy: 0.7706 - val_loss: 0.4449 - val_accuracy: 0.7955\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.4640 - accuracy: 0.7829 - val_loss: 0.4520 - val_accuracy: 0.7975\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.4563 - accuracy: 0.7885 - val_loss: 0.4656 - val_accuracy: 0.7775\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.4399 - accuracy: 0.7961 - val_loss: 0.4426 - val_accuracy: 0.7905\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.4341 - accuracy: 0.7990 - val_loss: 0.4528 - val_accuracy: 0.8030\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.4207 - accuracy: 0.8077 - val_loss: 0.4331 - val_accuracy: 0.7995\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.4148 - accuracy: 0.8092 - val_loss: 0.4143 - val_accuracy: 0.8155\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.3909 - accuracy: 0.8223 - val_loss: 0.4089 - val_accuracy: 0.8190\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.3920 - accuracy: 0.8250 - val_loss: 0.4034 - val_accuracy: 0.8150\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.3900 - accuracy: 0.8219 - val_loss: 0.4076 - val_accuracy: 0.8200\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 0.3816 - accuracy: 0.8273 - val_loss: 0.4321 - val_accuracy: 0.8100\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 0.3688 - accuracy: 0.8340 - val_loss: 0.4924 - val_accuracy: 0.7840\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.3632 - accuracy: 0.8361 - val_loss: 0.4072 - val_accuracy: 0.8190\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.3510 - accuracy: 0.8420 - val_loss: 0.4464 - val_accuracy: 0.8025\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.3444 - accuracy: 0.8503 - val_loss: 0.3802 - val_accuracy: 0.8315\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 0.3365 - accuracy: 0.8512 - val_loss: 0.3860 - val_accuracy: 0.8330\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.3360 - accuracy: 0.8495 - val_loss: 0.3891 - val_accuracy: 0.8290\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 32s 128ms/step - loss: 0.3224 - accuracy: 0.8585 - val_loss: 0.4075 - val_accuracy: 0.8265\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 0.3232 - accuracy: 0.8576 - val_loss: 0.4252 - val_accuracy: 0.8145\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.3144 - accuracy: 0.8595 - val_loss: 0.4058 - val_accuracy: 0.8280\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 32s 128ms/step - loss: 0.3109 - accuracy: 0.8634 - val_loss: 0.3772 - val_accuracy: 0.8345\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.3025 - accuracy: 0.8661 - val_loss: 0.3805 - val_accuracy: 0.8310\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.2947 - accuracy: 0.8681 - val_loss: 0.4216 - val_accuracy: 0.8185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27b8443e790>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the CNN on the Training set and evaluating it on the Test set\n",
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6a479",
   "metadata": {},
   "source": [
    "Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a938ba9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.preprocessing.image' has no attribute 'load_img'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image\n\u001b[1;32m----> 3\u001b[0m test_image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/single_prediction/cat_or_dog_1.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, target_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n\u001b[0;32m      4\u001b[0m test_image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mimg_to_array(test_image)\n\u001b[0;32m      5\u001b[0m test_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(test_image, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.preprocessing.image' has no attribute 'load_img'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f3cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
